{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"88FjEkljpQom"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n","from scipy.stats import f\n","\n","import statsmodels.api as sm\n","import seaborn as sns\n","\n","import matplotlib.pyplot as plt\n","import os"]},{"cell_type":"markdown","metadata":{"id":"pAbTVLHisUIu"},"source":["**FUNZIONI PER VALUTARE LE PREDIZIONI SUL TEST SET**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TACnVtvIsTFq"},"outputs":[],"source":["def evaluate_predictions(X_test, y_test, y_pred):\n","    # R-squared (R²)\n","    r2 = r2_score(y_test, y_pred)\n","\n","    # Mean Absolute Error (MAE)\n","    mae = mean_absolute_error(y_test, y_pred)\n","\n","    # Mean Squared Error (MSE)\n","    mse = mean_squared_error(y_test, y_pred)\n","\n","    # Root Mean Squared Error (RMSE)\n","    rmse = np.sqrt(mse)\n","\n","    # Mean Absolute Percentage Error (MAPE)\n","    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n","\n","    # Residual Standard Error (RSE)\n","    # Residuals are the differences between the true values and the predictions\n","    residuals = y_test - y_pred\n","    rse = np.sqrt(np.sum(residuals**2) / (len(y_test) - 2))  # For simple linear regression, degrees of freedom = n - 2\n","\n","    # Output all the results\n","    print(f\"R-squared (R²): {r2:.4f}\")\n","    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n","    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n","    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n","    print(f\"Mean Absolute Percentage Error (MAPE): {mape:.4f}%\")\n","    print(f\"Residual Standard Error (RSE): {rse:.4f}\")\n","    # Create the scatter plot\n","    plt.scatter(X_test, y_test, color='blue', marker='o', label='ground truth BHB values')\n","    plt.scatter(X_test, y_pred, color='red', marker='+', label='predicted BHB values')\n","    plt.axhline(y=1.2, color='green', linestyle='--', linewidth=2, label='Threshold per diagnosi')\n","    # Set labels and title\n","    plt.xlabel('Glu (mmol/L)')\n","    plt.ylabel('BHB (mmol/L)')\n","    plt.title('Scatter plot: ground truth and predictions on test set')\n","\n","    # Add a legend\n","    plt.legend()\n","\n","    # Display the plot\n","    plt.show()\n","\n","    # Create a histogram\n","    plt.figure(figsize=(8, 6))  # Adjust figure size if needed\n","    sns.histplot(residuals, kde=True, bins=30)  # kde=True adds a kernel density estimate\n","    plt.title('Distribuzione dei residui')\n","    plt.xlabel('Residuai')\n","    plt.ylabel('Frequenza')\n","    plt.show()\n","\n","    # Create a Q-Q plot (optional)\n","    sm.qqplot(residuals, line='45', fit=True)\n","    plt.title('Q-Q Plot dei residui')\n","    plt.show()\n","\n","\n","# Function to compute TP, TN, FP, FN\n","def compute_confusion_matrix(boolean_predictions, boolean_ground_truth):\n","    TP = np.sum((boolean_predictions == True) & (boolean_ground_truth == True))   # Both True\n","    TN = np.sum((boolean_predictions == False) & (boolean_ground_truth == False)) # Both False\n","    FP = np.sum((boolean_predictions == True) & (boolean_ground_truth == False))  # Predicted True, but False in ground truth\n","    FN = np.sum((boolean_predictions == False) & (boolean_ground_truth == True))  # Predicted False, but True in ground truth\n","    return TP, TN, FP, FN\n","\n","\n","def compute_classification_metrics(TP, TN, FP, FN):\n","  \"\"\"\n","  Computes accuracy, recall, and F1 score.\n","\n","  Args:\n","    TP: True positives.\n","    TN: True negatives.\n","    FP: False positives.\n","    FN: False negatives.\n","\n","  Returns:\n","    A tuple containing accuracy, recall, and F1 score.\n","  \"\"\"\n","  accuracy = (TP + TN) / (TP + TN + FP + FN)\n","  recall = TP / (TP + FN)\n","  precision = TP / (TP + FP)\n","  f1_score = 2 * (precision * recall) / (precision + recall)\n","\n","  return accuracy, recall, precision, f1_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BkwHYTIM7lvt"},"outputs":[],"source":["cvs_source= 'Holstein_diary_cows.csv'\n","df = pd.read_csv(cvs_source)"]},{"cell_type":"markdown","metadata":{"id":"aY-0I3Bk8aiH"},"source":["**Eseguire la prossima cella per visualizzare il dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4S5fD9AeqcMi"},"outputs":[],"source":["# Create the scatter plot\n","plt.scatter(df['Glu'], df['BHB'])\n","\n","# Set labels and title\n","plt.xlabel('Glu (mmol/L)')\n","plt.ylabel('BHB (mmol/L)')\n","plt.title('Scatter plot: Glu vs BHB')\n","\n","# Add a legend\n","plt.legend()\n","\n","# Display the plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"05sLXt4K8hTU"},"source":["**DIVIDI IL DATASET IN 80% TRAIN E 20% TEST**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tXmhvst18zn9"},"outputs":[],"source":["X = df[['Glu']]  # Features (independent variable)\n","y = df['BHB']    # Target (dependent variable)\n","\n","# Split data into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"IKnHfW6YFbMI"},"source":["**CREA ED ALLENA UN MODELLO DI RANDOM FOREST REGRESSION**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5KYnuYTMF4MQ"},"outputs":[],"source":["model = RandomForestRegressor(n_estimators=100, random_state=42)  # You can adjust hyperparameters\n","model.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{"id":"uFyWC0bsExlE"},"source":["**APPLICA IL MODELLO SUL TEST SET E VALUTA LE SUE PREDIZIONI**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g90IUpydE31H"},"outputs":[],"source":["y_pred = model.predict(X_test)\n","evaluate_predictions(X_test, y_test, y_pred)\n","\n","if not isinstance(y_test, np.ndarray):\n","  y_test = y_test.values\n","\n","threshold = 1.2  # The threshold value\n","\n","# Create boolean arrays\n","bool_y_test = y_test >= threshold\n","bool_y_pred = y_pred >= threshold\n","\n","# Compute metrics for each prediction array\n","TP, TN, FP, FN = compute_confusion_matrix(bool_y_pred, bool_y_test)\n","accuracy, recall, precision, f1_score = compute_classification_metrics(TP, TN, FP, FN)\n","\n","print(\"Matrice di confusione:\")\n","print(f\"TP: {TP}\")\n","print(f\"TN: {TN}\")\n","print(f\"FP: {FP}\")\n","print(f\"FN: {FN}\")\n","print(\"Metriche del classificatore:\")\n","print(f\"Accuratezza: {accuracy}\")\n","print(f\"Richiamo: {recall}\")\n","print(f\"Precisione: {precision}\")\n","print(f\"F1: {f1_score}\")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}